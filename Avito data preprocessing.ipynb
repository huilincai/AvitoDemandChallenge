{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "\n",
    "#natural language processing in Russian\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "\n",
    "#term frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#other\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import raw data\n",
    "training = pd.read_csv('train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testing = pd.read_csv('test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "traindex = training.index\n",
    "testdex = testing.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#feature engineering:price (training)\n",
    "training[\"price\"] = np.log(training[\"price\"]+0.001)\n",
    "training[\"price\"].fillna(-999,inplace=True)\n",
    "training[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "training[\"Weekday\"] = training['activation_date'].dt.weekday\n",
    "training[\"Weekd of Year\"] = training['activation_date'].dt.week\n",
    "training[\"Day of Month\"] = training['activation_date'].dt.day\n",
    "\n",
    "#feature engineerin:price (testing)\n",
    "testing[\"price\"] = np.log(testing[\"price\"]+0.001)\n",
    "testing[\"price\"].fillna(-999,inplace=True)\n",
    "testing[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "testing[\"Weekday\"] = testing['activation_date'].dt.weekday\n",
    "testing[\"Weekd of Year\"] = testing['activation_date'].dt.week\n",
    "testing[\"Day of Month\"] = testing['activation_date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create validation index, remove variables we won't use, and encode remaining variables\n",
    "#training\n",
    "training_index = training.loc[training.activation_date<=pd.to_datetime('2017-04-07')].index\n",
    "train_val_index = training.loc[training.activation_date>=pd.to_datetime('2017-04-08')].index\n",
    "training.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "\n",
    "categorical = [\"user_id\",\"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\"image_top_1\"]\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    training[col] = lbl.fit_transform(training[col].astype(str))\n",
    "\n",
    "#testing\n",
    "testing_index = testing.loc[testing.activation_date<=pd.to_datetime('2017-04-07')].index\n",
    "test_val_index = testing.loc[testing.activation_date>=pd.to_datetime('2017-04-08')].index\n",
    "testing.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "\n",
    "#categorical = [\"user_id\",\"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\"image_top_1\"] already defined above\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    testing[col] = lbl.fit_transform(testing[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature engineering: text (training)\n",
    "training['text_feat'] = training.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "training.drop([\"param_1\",\"param_2\",\"param_3\"],axis=1,inplace=True)\n",
    "\n",
    "#feature engineering: text (testing)\n",
    "testing['text_feat'] = testing.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "testing.drop([\"param_1\",\"param_2\",\"param_3\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#meta text features (training)\n",
    "textfeats = [\"description\",\"text_feat\", \"title\"]\n",
    "for cols in textfeats:\n",
    "    training[cols] = training[cols].astype(str) \n",
    "    training[cols] = training[cols].astype(str).fillna('nicapotato') # FILL NA\n",
    "    training[cols] = training[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    training[cols + '_num_chars'] = training[cols].apply(len) # Count number of Characters\n",
    "    training[cols + '_num_words'] = training[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    training[cols + '_num_unique_words'] = training[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    training[cols + '_words_vs_unique'] = training[cols+'_num_unique_words'] / training[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    \n",
    "#meta text features (testing)\n",
    "#textfeats = [\"description\",\"text_feat\", \"title\"], already defined above\n",
    "for cols in textfeats:\n",
    "    testing[cols] = testing[cols].astype(str) \n",
    "    testing[cols] = testing[cols].astype(str).fillna('nicapotato') # FILL NA\n",
    "    testing[cols] = testing[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    testing[cols + '_num_chars'] = testing[cols].apply(len) # Count number of Characters\n",
    "    testing[cols + '_num_words'] = testing[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    testing[cols + '_num_unique_words'] = testing[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    testing[cols + '_words_vs_unique'] = testing[cols+'_num_unique_words'] / testing[cols+'_num_words'] * 100 # Count Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set stopwords\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "\n",
    "#term frequency parameters\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": 'l2',\n",
    "    #\"min_df\":5,\n",
    "    #\"max_df\":.9,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "\n",
    "#define function to create term frequency variables\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "\n",
    "#create vectorized NLP variables\n",
    "vectorizer = FeatureUnion([\n",
    "        ('description',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=16000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('description'))),\n",
    "        ('text_feat',CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('text_feat'))),\n",
    "        ('title',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            **tfidf_para,\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('title')))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.to_csv('newtrain.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.to_csv('newtest.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
